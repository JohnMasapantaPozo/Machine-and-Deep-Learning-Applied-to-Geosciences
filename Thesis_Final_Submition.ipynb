{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis_Final_Submition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdYOaEW74X4o7FmUrK8XSq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnMasapantaPozo/Machine-and-Deep-Learning-Applied-to-Geosciences/blob/root/Thesis_Final_Submition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKcAATUEmd4M"
      },
      "source": [
        "#Importing libraries - NOTE: XGB model has to be run in a GPU, otherwise remove \"tree_method=gpu_hist\" from parameters.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import max_error, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, recall_score, precision_score, f1_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mg1jsA9rZxI",
        "outputId": "c76d75c3-61dc-4ded-f500-e59927fb559e"
      },
      "source": [
        "#Run if running in Google Cloud\n",
        "from google.colab import drive   #Getting access to Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6MTRXRosGz-"
      },
      "source": [
        "#Data Directory\n",
        "directory = '/content/drive/MyDrive/Thesis_data/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nQd9yf7s9oe"
      },
      "source": [
        "#Reading Raw Data\n",
        "def reading_data():\n",
        "  lithology_numbers = {30000: 0, \n",
        "                       65030: 1, \n",
        "                       65000: 2, \n",
        "                       80000: 3, \n",
        "                       74000: 4, \n",
        "                       70000: 5, \n",
        "                       70032: 6, \n",
        "                       88000: 7, \n",
        "                       86000: 8, \n",
        "                       99000: 9, \n",
        "                       90000: 10, \n",
        "                       93000: 11\n",
        "                       }\n",
        "\n",
        "  #training dataset\n",
        "  labeled_data = pd.read_csv(directory + 'train.csv', sep=';')\n",
        "  labeled_data = labeled_data.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY':'LITHO', \n",
        "                                              'FORCE_2020_LITHOFACIES_CONFIDENCE':'LIT_CONF'\n",
        "                                              })\n",
        "  labeled_data = labeled_data.drop(['LIT_CONF'], axis=1)\n",
        "  labeled_data['LITHO'] = labeled_data[\"LITHO\"].map(lithology_numbers)\n",
        "\n",
        "  #open dataset\n",
        "  test_data = pd.read_csv(directory + 'test.csv', sep=';')\n",
        "  test_labels = pd.read_csv(directory + 'test_target.csv', sep=';')\n",
        "  test_data = pd.merge(test_data, test_labels, on=['WELL', 'DEPTH_MD'])\n",
        "  test_data = test_data.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY':'LITHO'})\n",
        "  test_data['LITHO'] = test_data[\"LITHO\"].map(lithology_numbers)\n",
        "\n",
        "  #hidden dataset\n",
        "  hidden_data = pd.read_csv(directory + 'hidden_test.csv', sep=';')\n",
        "  hidden_data = hidden_data.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY':'LITHO'})\n",
        "  hidden_data = hidden_data.drop(['FORCE_2020_LITHOFACIES_CONFIDENCE'], axis=1)\n",
        "  hidden_data['LITHO'] = hidden_data['LITHO'].map(lithology_numbers)\n",
        "\n",
        "  return labeled_data, test_data, hidden_data\n",
        "\n",
        "def base_well_name(row):\n",
        "    well_name = row['WELL']\n",
        "    return well_name.split()[0]\n",
        "\n",
        "def data_preprocessing(training_data, test_data1, test_data2):\n",
        "\n",
        "  train_len  = training_data.shape[0]; train_well = training_data.WELL.values; train_depth = training_data.DEPTH_MD.values\n",
        "  test_len   = test_data1.shape[0];    test_well = test_data1.WELL.values;     test_depth = test_data1.DEPTH_MD.values\n",
        "  hidden_len = test_data2.shape[0];    hidden_well = test_data2.WELL.values;   hidden_depth = test_data2.DEPTH_MD.values\n",
        "\n",
        "  #concatenate datasets\n",
        "  df_concat = pd.concat((training_data, test_data1, test_data2)).reset_index(drop=True)\n",
        "\n",
        "  #dropping columns\n",
        "  drop_cols = ['SGR', 'ROPA', 'RXO', 'MUDWEIGHT']\n",
        "  df_drop = df_concat.drop(drop_cols, axis=1)\n",
        "\n",
        "  #encoding datasets\n",
        "  df_drop['GROUP_encoded'] = df_drop['GROUP'].astype('category')\n",
        "  df_drop['GROUP_encoded'] = df_drop['GROUP_encoded'].cat.codes\n",
        "\n",
        "  df_drop['FORMATION_encoded'] = df_drop['FORMATION'].astype('category')\n",
        "  df_drop['FORMATION_encoded'] = df_drop['FORMATION_encoded'].cat.codes\n",
        "\n",
        "  df_drop['WELL_encoded'] = df_drop['WELL'].astype('category')\n",
        "  df_drop['WELL_encoded'] = df_drop['WELL_encoded'].cat.codes\n",
        "\n",
        "  #clustering by well location\n",
        "  training_wells = training_data['WELL'].unique(); test_wells = test_data1['WELL'].unique(); hidden_wells = test_data2['WELL'].unique()\n",
        "\n",
        "  well_names = np.concatenate((training_wells, test_wells, hidden_wells))\n",
        "  well_names_df = pd.DataFrame({'WELL':well_names})\n",
        "\n",
        "  #importing metadata\n",
        "  well_meta_df = pd.read_csv('/content/drive/MyDrive/Thesis_data/wellbore_exploration_all.csv')\n",
        "  well_meta_df.rename(columns={'wlbWellboreName': 'WELL', \n",
        "                               'wlbWell': 'WELL_HEAD', \n",
        "                               'wlbNsDecDeg': 'lat', \n",
        "                               'wlbEwDesDeg': 'lon', \n",
        "                               'wlbDrillingOperator': 'Drilling_Operator', \n",
        "                               'wlbPurposePlanned': 'Purpose', \n",
        "                               'wlbCompletionYear': 'Completion_Year', \n",
        "                               'wlbFormationAtTd': 'Formation'\n",
        "                               }, inplace=True)\n",
        "  \n",
        "  well_locations_df = well_meta_df[['WELL_HEAD', 'lat', 'lon']].drop_duplicates(subset=['WELL_HEAD'])\n",
        "  well_meta_df = well_meta_df[['WELL', 'Drilling_Operator', 'Purpose', 'Completion_Year', 'Formation']]\n",
        "\n",
        "  well_names_df['WELL_HEAD'] = well_names_df.apply(lambda row: base_well_name(row), axis=1)\n",
        "  locations_df = well_names_df.merge(well_locations_df, how='inner', on='WELL_HEAD')\n",
        "  locations_df = locations_df.merge(well_meta_df, how='left', on='WELL')\n",
        "\n",
        "  #Labeling train and test wells\n",
        "  locations_df.loc[locations_df['WELL'].isin(training_wells), 'Dataset'] = 'Train'\n",
        "  locations_df.loc[locations_df['WELL'].isin(test_wells), 'Dataset'] = 'Test'\n",
        "  locations_df.loc[locations_df['WELL'].isin(hidden_wells), 'Dataset'] = 'Hidden'\n",
        "\n",
        "  LonLat_df =  locations_df.drop(['WELL', \n",
        "                                  'WELL_HEAD', \n",
        "                                  'Drilling_Operator', \n",
        "                                  'Purpose', \n",
        "                                  'Completion_Year', \n",
        "                                  'Formation', \n",
        "                                  'Dataset'], \n",
        "                                  axis=1)\n",
        "  \n",
        "  location = LonLat_df[['lon', 'lat']].values\n",
        "  kmeans = KMeans(n_clusters=3, init='k-means++', random_state=1)\n",
        "  labels = kmeans.fit_predict(location)\n",
        "\n",
        "  df_drop = df_drop.rename(columns={'WELL':'Cluster'})\n",
        "  clust_map = dict(zip(locations_df.WELL.values, labels))\n",
        "  df_drop['Cluster'] = df_drop['Cluster'].map(clust_map)\n",
        "  df_drop2 = df_drop.drop(['GROUP', 'FORMATION'], axis=1)\n",
        "\n",
        "  #spliting datasets\n",
        "  traindata = df_drop2[:train_len].copy()\n",
        "  testdata = df_drop2[train_len:(train_len+test_len)].copy()\n",
        "  hiddendata = df_drop2[(train_len+test_len):].copy()\n",
        "\n",
        "  return traindata, testdata, hiddendata"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iACOn7M8aLA"
      },
      "source": [
        "def data_augmentation(traindata, testdata, hiddendata):\n",
        "\n",
        "  ##1. PREDICTING DTS\n",
        "\n",
        "  print('-----------------------------------------PREDINCTING DTS------------------------------------------')\n",
        "\n",
        "  traindata_dts = traindata[traindata.DTS.notna()]\n",
        "  X_dts = traindata_dts.drop(['LITHO', 'DTS'], axis=1)\n",
        "  Y_dts = traindata_dts['DTS']\n",
        "  X_dts_inp = X_dts.apply(lambda x: x.fillna(x.median()), axis=0)    #Imputation\n",
        "  X_dts_train, X_dts_val, Y_dts_train, Y_dts_val = train_test_split(X_dts_inp, Y_dts, test_size=0.3, random_state=42)   #train validation split\n",
        "\n",
        "  print('Training set shape {} and validation set shape {}'.format(X_dts_train.shape, X_dts_val.shape))\n",
        "\n",
        "  model1000 = XGBRegressor('''\n",
        "  \n",
        "                                HIDEN PARAMETERS\n",
        "                           '''\n",
        "                          )\n",
        "  \n",
        "  model1000.fit(X_dts_train, Y_dts_train.values.ravel(), early_stopping_rounds=100, eval_set=[(X_dts_val, Y_dts_val)], verbose=100)\n",
        "  train_pred = model1000.predict(X_dts_train)\n",
        "  val_pred = model1000.predict(X_dts_val)\n",
        "\n",
        "  print('Train set root mean squared error:', np.sqrt(mean_squared_error(Y_dts_train, train_pred)))\n",
        "  print('Validation set root mean squared error:', np.sqrt(mean_squared_error(Y_dts_val, val_pred)))\n",
        "\n",
        "  print('--------------------------------Imputing DTS LOG by ML predictions--------------------------------')\n",
        "  # Filling nan values before predicting DTS\n",
        "  X_train_DTS = traindata.drop(['LITHO', 'DTS'], axis=1)\n",
        "  X_train_DTS2 = X_train_DTS.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_test_DTS = testdata.drop(['LITHO', 'DTS'], axis=1)\n",
        "  X_test_DTS2 = X_test_DTS.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_hidden_DTS = hiddendata.drop(['LITHO', 'DTS'], axis=1)\n",
        "  X_hidden_DTS2 = X_hidden_DTS.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  #Predicting DTS (COMPLETE DATASETS)\n",
        "  traindata['DTS_pred'] = model1000.predict(X_train_DTS2)\n",
        "  testdata['DTS_pred'] = model1000.predict(X_test_DTS2)\n",
        "  hiddendata['DTS_pred'] = model1000.predict(X_hidden_DTS2)\n",
        "\n",
        "  #Inputing nan values in DTS with DTS_PREDICTED\n",
        "  traindata['DTS_COMB'] = traindata['DTS']\n",
        "  traindata['DTS_COMB'].fillna(traindata['DTS_pred'], inplace=True)\n",
        "\n",
        "  testdata['DTS_COMB'] = testdata['DTS']\n",
        "  testdata['DTS_COMB'].fillna(testdata['DTS_pred'], inplace=True)\n",
        "\n",
        "  hiddendata['DTS_COMB'] = hiddendata['DTS']\n",
        "  hiddendata['DTS_COMB'].fillna(hiddendata['DTS_pred'], inplace=True)\n",
        "  \n",
        "  ##2. PREDICTING NPHI\n",
        "\n",
        "  print('----------------------------------------PREDINCTING NPHI-----------------------------------------')\n",
        "\n",
        "  traindata_nphi = traindata[traindata.NPHI.notna()]\n",
        "  X_nphi = traindata_nphi.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI'], axis=1)\n",
        "  Y_nphi = traindata_nphi['NPHI']\n",
        "  X_nphi_inp = X_nphi.apply(lambda x: x.fillna(x.median()), axis=0)     #Imputation\n",
        "  X_nphi_train, X_nphi_val, Y_nphi_train, Y_nphi_val = train_test_split(X_nphi_inp, Y_nphi, test_size=0.3, random_state=42)     #train-validation split\n",
        "\n",
        "  print('Training set sahpe {} and validation set shape {}'.format(X_nphi_train.shape, X_nphi_val.shape))\n",
        "\n",
        "  model2000 = XGBRegressor('''\n",
        "  \n",
        "                                HIDEN PARAMETERS\n",
        "                           '''\n",
        "                          )\n",
        "  \n",
        "  model2000.fit(X_nphi_train, Y_nphi_train.values.ravel(), early_stopping_rounds=100, eval_set=[(X_nphi_val, Y_nphi_val)], verbose=100)\n",
        "  train_pred = model2000.predict(X_nphi_train)\n",
        "  val_pred = model2000.predict(X_nphi_val)\n",
        "\n",
        "  print('Training set root mean squared error:', np.sqrt(mean_squared_error(Y_nphi_train, train_pred)))\n",
        "  print('Validation set root mean squared error:', np.sqrt(mean_squared_error(Y_nphi_val, val_pred)))\n",
        "\n",
        "  print('--------------------------------Imputing NPHI LOG by ML predictions--------------------------------')\n",
        "\n",
        "  # Filling nan values before predicting NPHI\n",
        "  X_train_NPHI = traindata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI'], axis=1)\n",
        "  X_train_NPHI2 = X_train_NPHI.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_test_NPHI = testdata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI'], axis=1)\n",
        "  X_test_NPHI2 = X_test_NPHI.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_hidden_NPHI = hiddendata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI'], axis=1)\n",
        "  X_hidden_NPHI2 = X_hidden_NPHI.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  #Predicting DTS (COMPLETE DATASETS)\n",
        "  traindata['NPHI_pred'] = model2000.predict(X_train_NPHI2)\n",
        "  testdata['NPHI_pred'] = model2000.predict(X_test_NPHI2)\n",
        "  hiddendata['NPHI_pred'] = model2000.predict(X_hidden_NPHI2)\n",
        "\n",
        "  #Inputing nan values in DTS with DTS_PREDICTED\n",
        "  traindata['NPHI_COMB'] = traindata['NPHI']\n",
        "  traindata['NPHI_COMB'].fillna(traindata['NPHI_pred'], inplace=True)\n",
        "\n",
        "  testdata['NPHI_COMB'] = testdata['NPHI']\n",
        "  testdata['NPHI_COMB'].fillna(testdata['NPHI_pred'], inplace=True)\n",
        "\n",
        "  hiddendata['NPHI_COMB'] = hiddendata['NPHI']\n",
        "  hiddendata['NPHI_COMB'].fillna(hiddendata['NPHI_pred'], inplace=True)\n",
        "\n",
        "  ##3. PREDICTING RHOB\n",
        "  print('----------------------------------------PREDINCTING RHOB-----------------------------------------')\n",
        "\n",
        "  traindata_rhob = traindata[traindata.RHOB.notna()]\n",
        "  X_rhob = traindata_rhob.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB'], axis=1)\n",
        "  Y_rhob = traindata_rhob['RHOB']\n",
        "  X_rhob_inp = X_rhob.apply(lambda x: x.fillna(x.median()), axis=0)    #Imputation\n",
        "  X_rhob_train, X_rhob_val, Y_rhob_train, Y_rhob_val = train_test_split(X_rhob_inp, Y_rhob, test_size=0.3, random_state=42)  #train-validation split\n",
        "\n",
        "  print('Training set sahpe {} and validation set shape {}'.format(X_rhob_train.shape, X_rhob_val.shape))\n",
        "\n",
        "  model4000 = XGBRegressor('''\n",
        "  \n",
        "                                HIDEN PARAMETERS\n",
        "                           '''\n",
        "                          )\n",
        "  \n",
        "  model4000.fit(X_rhob_train, Y_rhob_train.values.ravel(), early_stopping_rounds=100, eval_set=[(X_rhob_val, Y_rhob_val)], verbose=100)\n",
        "  train_pred = model4000.predict(X_rhob_train)\n",
        "  val_pred = model4000.predict(X_rhob_val)\n",
        "\n",
        "  print('Train set root mean squared error:', np.sqrt(mean_squared_error(Y_rhob_train, train_pred)))\n",
        "  print('Validation set root mean squared error:', np.sqrt(mean_squared_error(Y_rhob_val, val_pred)))\n",
        "  \n",
        "  print('--------------------------------Imputing RHOB LOG by ML predictions--------------------------------')\n",
        "\n",
        "  # Filling nan values before predicting NPHI\n",
        "  X_train_RHOB = traindata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB'], axis=1)\n",
        "  X_train_RHOB2 = X_train_RHOB.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_test_RHOB = testdata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB'], axis=1)\n",
        "  X_test_RHOB2 = X_test_RHOB.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_hidden_RHOB = hiddendata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB'], axis=1)\n",
        "  X_hidden_RHOB2 = X_hidden_RHOB.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  #Predicting DTS (COMPLETE DATASETS)\n",
        "  traindata['RHOB_pred'] = model4000.predict(X_train_RHOB2)\n",
        "  testdata['RHOB_pred'] = model4000.predict(X_test_RHOB2)\n",
        "  hiddendata['RHOB_pred'] = model4000.predict(X_hidden_RHOB2)\n",
        "\n",
        "  #Inputing nan values in DTS with DTS_PREDICTED\n",
        "  traindata['RHOB_COMB'] = traindata['RHOB']\n",
        "  traindata['RHOB_COMB'].fillna(traindata['RHOB_pred'], inplace=True)\n",
        "\n",
        "  testdata['RHOB_COMB'] = testdata['RHOB']\n",
        "  testdata['RHOB_COMB'].fillna(testdata['RHOB_pred'], inplace=True)\n",
        "\n",
        "  hiddendata['RHOB_COMB'] = hiddendata['RHOB']\n",
        "  hiddendata['RHOB_COMB'].fillna(hiddendata['RHOB_pred'], inplace=True)\n",
        "\n",
        "  ##4. PREDICTING DTC\n",
        "  print('----------------------------------------PREDINCTING DTC-----------------------------------------')\n",
        "\n",
        "  traindata_dtc = traindata[traindata.DTC.notna()]\n",
        "  X_dtc = traindata_dtc.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB', 'RHOB_pred', 'DTC'], axis=1)\n",
        "  Y_dtc = traindata_dtc['DTC']\n",
        "  X_dtc_inp = X_dtc.apply(lambda x: x.fillna(x.median()), axis=0)      #Imputation\n",
        "  X_dtc_train, X_dtc_val, Y_dtc_train, Y_dtc_val = train_test_split(X_dtc_inp, Y_dtc, test_size=0.3, random_state=42)     #Spliting train-validation\n",
        "\n",
        "  print('Training set sahpe {} and validation set shape {}'.format(X_dtc_train.shape, X_dtc_val.shape))\n",
        "\n",
        "  model3000 = XGBRegressor('''\n",
        "  \n",
        "                                HIDEN PARAMETERS\n",
        "                           '''\n",
        "                          )\n",
        "  \n",
        "  model3000.fit(X_dtc_train, Y_dtc_train.values.ravel(), early_stopping_rounds=100, eval_set=[(X_dtc_val, Y_dtc_val)], verbose=100)\n",
        "  train_pred = model3000.predict(X_dtc_train)\n",
        "  val_pred = model3000.predict(X_dtc_val)\n",
        "\n",
        "  print('Train set root mean squared error:', np.sqrt(mean_squared_error(Y_dtc_train, train_pred)))\n",
        "  print('Validation set root mean squared error:', np.sqrt(mean_squared_error(Y_dtc_val, val_pred)))\n",
        "\n",
        "  print('--------------------------------Imputing DTC LOG by ML predictions--------------------------------')\n",
        "\n",
        "  # Filling nan values before predicting NPHI\n",
        "  X_train_DTC = traindata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB', 'RHOB_pred', 'DTC'], axis=1)\n",
        "  X_train_DTC2 = X_train_DTC.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_test_DTC = testdata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB', 'RHOB_pred', 'DTC'], axis=1)\n",
        "  X_test_DTC2 = X_test_DTC.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  X_hidden_DTC = hiddendata.drop(['LITHO', 'DTS', 'DTS_pred', 'NPHI', 'NPHI_pred', 'RHOB', 'RHOB_pred', 'DTC'], axis=1)\n",
        "  X_hidden_DTC2 = X_hidden_DTC.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  #Predicting DTS (COMPLETE DATASETS)\n",
        "  traindata['DTC_pred'] = model3000.predict(X_train_DTC2)\n",
        "  testdata['DTC_pred'] = model3000.predict(X_test_DTC2)\n",
        "  hiddendata['DTC_pred'] = model3000.predict(X_hidden_DTC2)\n",
        "\n",
        "  #Inputing nan values in DTS with DTS_PREDICTED\n",
        "  traindata['DTC_COMB'] = traindata['DTC']\n",
        "  traindata['DTC_COMB'].fillna(traindata['DTC_pred'], inplace=True)\n",
        "\n",
        "  testdata['DTC_COMB'] = testdata['DTC']\n",
        "  testdata['DTC_COMB'].fillna(testdata['DTC_pred'], inplace=True)\n",
        "\n",
        "  hiddendata['DTC_COMB'] = hiddendata['DTC']\n",
        "  hiddendata['DTC_COMB'].fillna(hiddendata['DTC_pred'], inplace=True)\n",
        "\n",
        "  #additional features\n",
        "  print('--------------------------------Creating additional features--------------------------------')\n",
        "  #Train Set\n",
        "  traindata['AI'] = traindata.RHOB * (1e6/traindata.DTS_COMB)\n",
        "  traindata['AI_P'] = traindata.RHOB * (1e6/traindata.DTC)\n",
        "  traindata['DT_R'] = traindata.DTC / traindata.DTS_COMB\n",
        "  traindata['GM'] = ((1e6/traindata.DTS_COMB)**2) * traindata.RHOB\n",
        "  traindata['K'] = (((1e6/traindata.DTC)**2) * traindata.RHOB) - (4 * traindata.GM/3)\n",
        "  traindata['MD_TVD'] = -(traindata.DEPTH_MD/traindata.Z_LOC)\n",
        "\n",
        "  #Test Set\n",
        "  testdata['AI'] = testdata.RHOB * (1e6/testdata.DTS_COMB)\n",
        "  testdata['AI_P'] = testdata.RHOB * (1e6/testdata.DTC)\n",
        "  testdata['DT_R'] = testdata.DTC / testdata.DTS_COMB\n",
        "  testdata['GM'] = ((1e6/testdata.DTS_COMB)**2) * testdata.RHOB\n",
        "  testdata['K'] = (((1e6/testdata.DTC)**2) * testdata.RHOB) - (4 * testdata.GM/3)\n",
        "  testdata['MD_TVD'] = -(testdata.DEPTH_MD/testdata.Z_LOC)\n",
        "\n",
        "  #Hidden Set\n",
        "  hiddendata['AI'] = hiddendata.RHOB * (1e6/hiddendata.DTS_COMB)\n",
        "  hiddendata['AI_P'] = hiddendata.RHOB * (1e6/hiddendata.DTC)\n",
        "  hiddendata['DT_R'] = hiddendata.DTC / hiddendata.DTS_COMB\n",
        "  hiddendata['GM'] = ((1e6/hiddendata.DTS_COMB)**2) * hiddendata.RHOB\n",
        "  hiddendata['K'] = (((1e6/hiddendata.DTC)**2) * hiddendata.RHOB) - (4 * hiddendata.GM/3)\n",
        "  hiddendata['MD_TVD'] = -(hiddendata.DEPTH_MD/hiddendata.Z_LOC)\n",
        "\n",
        "  #dropping unnecessary data\n",
        "  cols_drop = ['DTS_pred', 'NPHI_pred', 'RHOB_pred', 'DTC_pred']\n",
        "  traindata = traindata.drop(cols_drop, axis=1)\n",
        "  testdata = testdata.drop(cols_drop, axis=1)\n",
        "  hiddendata = hiddendata.drop(cols_drop, axis=1)\n",
        "\n",
        "  print('Features included in the datasets: {}'.format(traindata.columns))\n",
        "\n",
        "  return traindata, testdata, hiddendata\n",
        "\n",
        "def normalization(traindata, testdata, hiddendata):\n",
        "\n",
        "  train_features = traindata.drop(['LITHO'], axis=1);   train_labels = traindata['LITHO']\n",
        "  test_features = testdata.drop(['LITHO'], axis=1);     test_labels = testdata['LITHO']\n",
        "  hidden_features = hiddendata.drop(['LITHO'], axis=1); hidden_labels = hiddendata['LITHO']\n",
        "\n",
        "  train_features_inp = train_features.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "  test_features_inp = test_features.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "  hidden_features_inp = hidden_features.apply(lambda x: x.fillna(x.median()), axis=0)\n",
        "\n",
        "  n = train_features_inp.shape[1]\n",
        "  std = StandardScaler()\n",
        "  x_train_std = train_features_inp.copy()\n",
        "  x_test_std = test_features_inp.copy()\n",
        "  x_hidden_std = hidden_features_inp.copy()\n",
        "\n",
        "  x_train_std.iloc[:,:n] = std.fit_transform(x_train_std.iloc[:,:n])\n",
        "  x_test_std.iloc[:,:n] = std.transform(x_test_std.iloc[:,:n])\n",
        "  x_hidden_std.iloc[:,:n] = std.transform(x_hidden_std.iloc[:,:n])\n",
        "\n",
        "  cleaned_traindata = pd.concat([x_train_std, train_labels], axis=1)\n",
        "  cleaned_testdata = pd.concat([x_test_std, test_labels], axis=1)\n",
        "  cleaned_hiddendata = pd.concat([x_hidden_std, hidden_labels], axis=1)\n",
        "\n",
        "  return cleaned_traindata, cleaned_testdata, cleaned_hiddendata\n",
        "\n",
        "# Evaluate prediction\n",
        "A = np.load(directory + 'penalty_matrix.npy')\n",
        "def score(y_true, y_pred):\n",
        "    S = 0.0\n",
        "    y_true = y_true.astype(int)\n",
        "    y_pred = y_pred.astype(int)\n",
        "    for i in range(0, y_true.shape[0]):\n",
        "        S -= A[y_true[i], y_pred[i]]\n",
        "    return S/y_true.shape[0]\n",
        "\n",
        "def predict_litho(clean_traindata, clean_testdata, clean_hiddendata):\n",
        "  #Recursive Feature Selected Features\n",
        "  best_features = ['RDEP','GR', 'NPHI_COMB', 'GM', 'AI_P', 'AI', 'DTC', 'DTS_COMB', 'RSHA', 'DT_R', 'RHOB', \n",
        "                  'K', 'DCAL', 'Y_LOC', 'Cluster', 'GROUP_encoded', 'WELL_encoded', 'FORMATION_encoded', \n",
        "                  'DEPTH_MD', 'Z_LOC', 'CALI', 'BS', 'X_LOC', 'RMED', 'PEF', 'SP', #'MD_TVD', 'ROP','RMIC','DRHO'\n",
        "                  ]\n",
        "  #selecting training and test data\n",
        "  x_train = clean_traindata[best_features]; y_train = clean_traindata['LITHO']\n",
        "  x_test = clean_testdata[best_features]; y_test = clean_testdata['LITHO']\n",
        "  x_hidden = clean_hiddendata[best_features]; y_hidden = clean_hiddendata['LITHO']\n",
        "\n",
        "  #Stratified Cross-validation\n",
        "  split = 10\n",
        "  kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
        "\n",
        "  train_prob_xgb1 = np.zeros((len(x_train), 12))\n",
        "  open_prob_xgb1 = np.zeros((len(x_test), 12))\n",
        "  hidden_prob_xgb1 = np.zeros((len(x_hidden), 12))\n",
        "\n",
        "  xgbmodel_noarg = XGBClassifier('''\n",
        "  \n",
        "                                HIDEN PARAMETERS'''\n",
        "\n",
        "                                )\n",
        "  i = 1\n",
        "  for (train_index, test_index) in kf.split(x_train, y_train):\n",
        "    X_train, X_test = x_train.iloc[train_index], x_train.iloc[test_index]\n",
        "    Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    xgbmodel_noarg.fit(X_train, Y_train.values.ravel(), early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
        "    prediction = xgbmodel_noarg.predict(X_test)\n",
        "    print('Fold accuracy:', accuracy_score(Y_test, prediction))\n",
        "\n",
        "    print(f'-----------------------FOLD {i}---------------------')\n",
        "    \n",
        "    train_prob_xgb1 += xgbmodel_noarg.predict_proba(x_train)\n",
        "    open_prob_xgb1 += xgbmodel_noarg.predict_proba(x_test)\n",
        "    hidden_prob_xgb1 += xgbmodel_noarg.predict_proba(x_hidden)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  train_prob_xgb1 = pd.DataFrame(train_prob_xgb1/split)\n",
        "  train_pred_xgb1 = np.array(pd.DataFrame(train_prob_xgb1).idxmax(axis=1))\n",
        "\n",
        "  open_prob_xgb1 = pd.DataFrame(open_prob_xgb1/split)\n",
        "  open_pred_xgb1 = np.array(pd.DataFrame(open_prob_xgb1).idxmax(axis=1))\n",
        "\n",
        "  hidden_prob_xgb1 = pd.DataFrame(hidden_prob_xgb1/split)\n",
        "  hidden_pred_xgb1 = np.array(pd.DataFrame(hidden_prob_xgb1).idxmax(axis=1))\n",
        "\n",
        "  #Printing Reports \n",
        "  print('-----------------------TRAIN SET REPORT---------------------')\n",
        "  print(\"Open set RMSE:\", np.sqrt(mean_squared_error(y_train, train_pred_xgb1)))\n",
        "  print('Open set penalty matrix score:', score(y_train.values, train_pred_xgb1))\n",
        "  print('Open set report:', classification_report(y_train, train_pred_xgb1))\n",
        "  print('-----------------------OPEN SET REPORT---------------------')\n",
        "  print(\"Open set RMSE:\", np.sqrt(mean_squared_error(y_test, open_pred_xgb1)))\n",
        "  print('Open set penalty matrix score:', score(y_test.values, open_pred_xgb1))\n",
        "  print('Open set report:', classification_report(y_test, open_pred_xgb1))\n",
        "  print('-----------------------HIDDEN SET REPORT---------------------')\n",
        "  print(\"Hidden set RMSE:\", np.sqrt(mean_squared_error(y_hidden, hidden_pred_xgb1)))\n",
        "  print('Hidden set penalty matrix score:', score(y_hidden.values, hidden_pred_xgb1))\n",
        "  print('Hidden set report:', classification_report(y_hidden, hidden_pred_xgb1))\n",
        "\n",
        "def run_model():\n",
        "  xxx, yyy, zzz = reading_data()\n",
        "  traindata, testdata, hiddendata = data_preprocessing(xxx, yyy, zzz)\n",
        "  a,b,c = data_augmentation(traindata, testdata, hiddendata)\n",
        "  q, p, t = normalization(a, b, c)\n",
        "  predict_litho(q, p, t)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyveEyb0vE-2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxitKfWrHS1M",
        "outputId": "107e9e96-0dcf-43bc-b7a6-bc9b64785614"
      },
      "source": [
        "## Run the model\n",
        "run_model()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------PREDINCTING DTS------------------------------------------\n",
            "Training set shape (122229, 23) and validation set shape (52384, 23)\n",
            "[05:53:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:194.561\n",
            "Will train until validation_0-rmse hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-rmse:15.8819\n",
            "Train set root mean squared error: 15.833977397169491\n",
            "Validation set root mean squared error: 15.881878499415139\n",
            "--------------------------------Imputing DTS LOG by ML predictions--------------------------------\n",
            "----------------------------------------PREDINCTING NPHI-----------------------------------------\n",
            "Training set sahpe (535786, 23) and validation set shape (229623, 23)\n",
            "[05:53:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:0.194089\n",
            "Will train until validation_0-rmse hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-rmse:0.052543\n",
            "Training set root mean squared error: 0.05226648031075551\n",
            "Validation set root mean squared error: 0.052542810465928014\n",
            "--------------------------------Imputing NPHI LOG by ML predictions--------------------------------\n",
            "----------------------------------------PREDINCTING RHOB-----------------------------------------\n",
            "Training set sahpe (706469, 23) and validation set shape (302773, 23)\n",
            "[05:55:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:1.62403\n",
            "Will train until validation_0-rmse hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-rmse:0.091064\n",
            "Train set root mean squared error: 0.09103539923378885\n",
            "Validation set root mean squared error: 0.09106429168438492\n",
            "--------------------------------Imputing RHOB LOG by ML predictions--------------------------------\n",
            "----------------------------------------PREDINCTING DTC-----------------------------------------\n",
            "Training set sahpe (762753, 23) and validation set shape (326895, 23)\n",
            "[05:56:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:105.123\n",
            "Will train until validation_0-rmse hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-rmse:5.24272\n",
            "Train set root mean squared error: 5.219325107053688\n",
            "Validation set root mean squared error: 5.242719196352279\n",
            "--------------------------------Imputing DTC LOG by ML predictions--------------------------------\n",
            "--------------------------------Creating additional features--------------------------------\n",
            "Features included in the datasets: Index(['Cluster', 'DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA',\n",
            "       'RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP',\n",
            "       'DTS', 'DCAL', 'DRHO', 'RMIC', 'LITHO', 'GROUP_encoded',\n",
            "       'FORMATION_encoded', 'WELL_encoded', 'DTS_COMB', 'NPHI_COMB',\n",
            "       'RHOB_COMB', 'DTC_COMB', 'AI', 'AI_P', 'DT_R', 'GM', 'K', 'MD_TVD'],\n",
            "      dtype='object')\n",
            "[0]\tvalidation_0-mlogloss:2.16519\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.335097\n",
            "Fold accuracy: 0.8914157810204012\n",
            "-----------------------FOLD 1---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16484\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.335024\n",
            "Fold accuracy: 0.8914575697772765\n",
            "-----------------------FOLD 2---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16439\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.333791\n",
            "Fold accuracy: 0.890714304021324\n",
            "-----------------------FOLD 3---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16478\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.33163\n",
            "Fold accuracy: 0.8927134326062999\n",
            "-----------------------FOLD 4---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16428\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.332611\n",
            "Fold accuracy: 0.894302483532819\n",
            "-----------------------FOLD 5---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16498\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.337642\n",
            "Fold accuracy: 0.8884076171925058\n",
            "-----------------------FOLD 6---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16466\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.333542\n",
            "Fold accuracy: 0.8924913072079692\n",
            "-----------------------FOLD 7---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16489\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.334052\n",
            "Fold accuracy: 0.8921922922486779\n",
            "-----------------------FOLD 8---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16466\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.333233\n",
            "Fold accuracy: 0.8925511101998275\n",
            "-----------------------FOLD 9---------------------\n",
            "[0]\tvalidation_0-mlogloss:2.16519\n",
            "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
            "[99]\tvalidation_0-mlogloss:0.337189\n",
            "Fold accuracy: 0.8902700532246628\n",
            "-----------------------FOLD 10---------------------\n",
            "-----------------------TRAIN SET REPORT---------------------\n",
            "Open set RMSE: 0.8330719392472656\n",
            "Open set penalty matrix score: -0.2800487351250864\n",
            "Open set report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86    168937\n",
            "           1       0.82      0.69      0.75    150455\n",
            "           2       0.92      0.97      0.94    720803\n",
            "           3       0.86      0.75      0.80     33329\n",
            "           4       0.78      0.10      0.18      1688\n",
            "           5       0.86      0.68      0.76     56320\n",
            "           6       0.92      0.89      0.91     10513\n",
            "           7       0.99      0.99      0.99      8213\n",
            "           8       0.93      0.86      0.89      1085\n",
            "           9       0.85      0.87      0.86     15245\n",
            "          10       0.87      0.46      0.60      3820\n",
            "          11       1.00      0.45      0.62       103\n",
            "\n",
            "    accuracy                           0.89   1170511\n",
            "   macro avg       0.89      0.71      0.76   1170511\n",
            "weighted avg       0.89      0.89      0.89   1170511\n",
            "\n",
            "-----------------------OPEN SET REPORT---------------------\n",
            "Open set RMSE: 1.0936945352630496\n",
            "Open set penalty matrix score: -0.5455839778924744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Open set report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81     24048\n",
            "           1       0.56      0.28      0.37     17558\n",
            "           2       0.83      0.94      0.88     83975\n",
            "           3       0.71      0.14      0.24      3306\n",
            "           4       0.00      0.00      0.00       416\n",
            "           5       0.52      0.52      0.52      4798\n",
            "           6       0.00      0.00      0.00       625\n",
            "           8       0.00      0.00      0.00       125\n",
            "           9       0.77      0.54      0.63      1245\n",
            "          10       0.83      0.41      0.55       690\n",
            "\n",
            "    accuracy                           0.79    136786\n",
            "   macro avg       0.50      0.37      0.40    136786\n",
            "weighted avg       0.76      0.79      0.76    136786\n",
            "\n",
            "-----------------------HIDDEN SET REPORT---------------------\n",
            "Hidden set RMSE: 1.0298652343272097\n",
            "Hidden set penalty matrix score: -0.43702562154301167\n",
            "Hidden set report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79     14045\n",
            "           1       0.70      0.45      0.55     12283\n",
            "           2       0.88      0.95      0.91     71827\n",
            "           3       0.29      0.23      0.25      4396\n",
            "           4       0.93      0.19      0.32       287\n",
            "           5       0.66      0.63      0.64      8374\n",
            "           6       0.73      0.41      0.52      2905\n",
            "           7       0.99      0.99      0.99      6498\n",
            "           8       0.76      0.61      0.68       597\n",
            "           9       0.71      0.35      0.47       941\n",
            "          10       0.85      0.54      0.66       244\n",
            "\n",
            "    accuracy                           0.82    122397\n",
            "   macro avg       0.75      0.56      0.62    122397\n",
            "weighted avg       0.81      0.82      0.81    122397\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}